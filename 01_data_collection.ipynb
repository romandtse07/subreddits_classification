{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this series of notebooks, we explore the idea of potentially creating an automated NLP based model that can learn what constitutes behavior worthy of a subreddit ban and apply this to future posts and communities.  Note that we limit our attention to r/incels and r/foreveralone comments at present with the assumption that if we can get high accuracy predicting comments between these two threads, we could get possibly better accuracy comparing incels to an unrelated subreddit.  Later, we also limit the amount of jargon present in an effort to generalize the model to other subreddits.  Though this process should always involve human judgement to a larger extent, we might use the models obtained here to detect smaller groups either starting new subreddits or permeating through existing subreddits, potentially stemming toxic behavior before it is enters the community.\n",
    "\n",
    "To this end, we use three notebooks to:\n",
    "1. Gather posts to be used later.\n",
    "2. Explore raw frequency of words to explore how well a simple bag-of-words approach might help us solve the problem.\n",
    "3. Introduce several changes to 2., including rescaling of our numerical representation of words as well as using a different classification method.  Though it is therefore difficult to determine if it is the rescaling or the new classifier that is responsible for the change in result, we simply note the increased accuracy in distinguishing the two groups and continue the discussion from there."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "focus": false,
    "id": "69b9a648-bcc7-490d-9f9b-ea244d156bd6"
   },
   "source": [
    "## Gathering Reddit Data from Pushshift API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-23T19:28:02.619411Z",
     "start_time": "2017-10-23T19:28:02.600856Z"
    }
   },
   "source": [
    "Here, we document the process used to interact with the pushshift API.  The database has collected a large volume of reddit content, which are used in the main notebook.  More information about the database, including query parameters and update schedule, can be found on https://pushshift.io/."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "focus": false,
    "id": "a948d79c-5527-4c0d-ab23-f5d43ce72056"
   },
   "source": [
    "## Pooling Data from the API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by defining a function used to collect either submissions or comments from the API.  The maximum number of items retreived at a time is 25.  A separate function was added to collect results over multiple files.  The function is limited to retreiving the most latest comments at the moment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "from pathlib import Path\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_r_json(subreddit, suffix, com_or_submit = 'submission', collect_n_ish=1000, start_time = None):\n",
    "    filename = f'./jsons/{subreddit}_{com_or_submit}_{suffix}.json'\n",
    "    if Path(filename).is_file():\n",
    "        print(f'{subreddit}_{com_or_submit}_{suffix}.json already exists.  Please delete to replace.')\n",
    "        return\n",
    "    \n",
    "    if com_or_submit not in ['submission', 'comment']:\n",
    "        print('com_or_submit(second argument) needs to be either \"submission\" or \"comment\" from subreddit.')\n",
    "        return\n",
    "    \n",
    "    if start_time:\n",
    "        URL = f\"https://api.pushshift.io/reddit/{com_or_submit}/search/?subreddit={subreddit}&before={start_time}\"\n",
    "    else:\n",
    "        URL = f\"https://api.pushshift.io/reddit/{com_or_submit}/search/?subreddit={subreddit}\"\n",
    "    header = {'User-agent':'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_13_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/96.0.3253.633 Safari/537.36'}\n",
    "    results = requests.get(URL, headers=header)\n",
    "    final = results.json()\n",
    "    \n",
    "\n",
    "    before = final['data'][len(final)-1]['created_utc']\n",
    "    \n",
    "    for i in range((collect_n_ish-25)//25):\n",
    "        URL = f\"https://api.pushshift.io/reddit/{com_or_submit}/search/?subreddit={subreddit}&before={before}\"\n",
    "        results = requests.get(URL, headers=header)\n",
    "        data = results.json()\n",
    "        final['data'].extend(data['data'])\n",
    "        before = data['data'][len(data['data'])-1]['created_utc']\n",
    "        if len(data['data']) < 25:\n",
    "            print('came up short')\n",
    "            break\n",
    "        time.sleep(1)\n",
    "            \n",
    "    try:\n",
    "        f = open(filename, 'w')\n",
    "    except:\n",
    "        Path(filename).touch()\n",
    "        f = open(filename, 'w')\n",
    "    \n",
    "    json.dump(final, f)\n",
    "    \n",
    "    f.close()\n",
    "    return before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_N_files(subreddits, com_or_sub, N_files):\n",
    "    start_time = None\n",
    "    for topic in subreddits:\n",
    "        for key in com_or_sub:\n",
    "            for i in range(N_files):\n",
    "                start_time = collect_r_json(topic, i, key, com_or_sub[key], start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics = ['incels', 'foreveralone', 'uncensorednews', 'altnewz', 'changemyview']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "com_or_sub = {'comment':10000, 'submission':2000}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collect_N_files(topics, com_or_sub, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A few more forums are collected here.  These two are assumed to contain some of the behavior of incels, though only truecels was banned at the time this sentence was typed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "collect_N_files(['truecels', 'mensrights'], com_or_sub, 1)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
